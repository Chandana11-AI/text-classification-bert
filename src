# -*- coding: utf-8 -*-
"""Copy of Text-Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11igBJm0-bLA0K41uomlRVoivJtrH_jYt
"""

!pip install transformers[torch]
!pip install accelatare
!pip install transformers opendatasets

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
import tensorflow as tf
from transformers import BertTokenizer
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

import opendatasets as od

od.download(
    "https://www.kaggle.com/datasets/jensenbaxter/10dataset-text-document-classification")

!pip install datasets

from datasets import load_dataset

dataset = load_dataset("10dataset-text-document-classification")

folder_path = "10dataset-text-document-classification"
folders = os.listdir(folder_path)
folders

rows = []
for c in folders:
    docs = os.listdir(f"{folder_path}/{c}")
    for d in docs:
        with open(f"{folder_path}/{c}/{d}", "r") as file:
            rows.append([file.read(), c])

import nltk
 nltk.download('stopwords')
 nltk.download('punkt')

from nltk.corpus import stopwords


stop_words = stopwords.words('english')
prt = nltk.stem.PorterStemmer()

def preprocess(document_path):

    with open(document_path, 'r') as file:
        document = file.read()

    #tokens = nltk.word_tokenize(document)

    #tokens_pun_lower = [i.lower() for i in tokens if i.isalnum()]

    #tokens_stop = [i for i in tokens_pun_lower if i not in stop_words]

    #terms = [prt.stem(i) for i in tokens_stop]

   # return " ".join(terms)
    return document
import os
Data = []
label=0
for dirname, _, filenames in os.walk(folder_path):
    for filename in filenames:
        #print(os.path.join(dirname, filename))

        doc_class = filename.split('_')[0].lower()
        doc_titles = filename
        documents = preprocess(os.path.join(dirname, filename))


        Data.append([doc_titles, documents, doc_class])

df = pd.DataFrame (Data, columns = ['Title', 'Document', 'Category'])

df.sample(30)

df2=pd.read_json("combined_train_unprompted.json",lines=True)

df3=pd.read_json("combined_val_unprompted.json",lines=True)

df2



df['count'] = df['Document'].apply(lambda x: len(x.split()))

category_count = df["Category"].value_counts()
categories = category_count.index
categories

category_count

labels = df['Category'].unique().tolist()
NUM_LABELS= len(labels)

id2label={id:label for id,label in enumerate(labels)}

label2id={label:id for id,label in enumerate(labels)}

df["labels"]=df.Category.map(lambda x: label2id[x.strip()])



df.head()

id2label

label2id

!pip install evaluate

import evaluate

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)

from transformers import BertForSequenceClassification,BertTokenizerFast

import torch
from torch.utils.data import Dataset
from torch import cuda
device = 'cuda' if cuda.is_available() else 'cpu'
device

model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=NUM_LABELS, id2label=id2label, label2id=label2id)
model.to(device)

df1=df.sample(frac=1)
train=df1.filter(['Document', 'labels'], axis=1).iloc[0:int(0.8*len(df))]
valid=df1.filter(['Document', 'labels'], axis=1).iloc[int(0.8*len(df)):int(0.9*len(df))]
test=df1.filter(['Document', 'labels'], axis=1).iloc[int(0.9*len(df)):]

df2.columns=train.columns
train_merged = pd.concat([train, df2], ignore_index=True)

train_merged

df3.columns=valid.columns
valid_merged = pd.concat([valid, df3.filter(['Document', 'labels'], axis=1).iloc[0:int(0.5*len(df3))]], ignore_index=True)

valid_merged

test_merged=pd.concat([test,df3.filter(['Document','labels'],axis=1).iloc[int(0.5*len(df3)):]],ignore_index=True)

test_merged

train_texts=train_merged["Document"]
train_labels=list(train["labels"])

valid_texts=valid_merged["Document"]
valid_labels=list(valid["labels"])


test_texts=test_merged["Document"]
test_labels=list(test["labels"])

train_texts

valid_texts

test_texts

tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased", max_length=512)

train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)
val_encodings  = tokenizer(list(valid_texts), truncation=True, padding=True)
test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)

class DataLoader(Dataset):

    def __init__(self, encodings, labels):

        self.encodings = encodings
        self.labels = labels
    def __getitem__(self, idx):

        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}

        item['labels'] = torch.tensor(self.labels[idx])
        return item


    def __len__(self):

        return len(self.labels)

train_dataloader = DataLoader(train_encodings, train_labels)

val_dataloader = DataLoader(val_encodings, valid_labels)

test_dataset = DataLoader(test_encodings, test_labels)

from transformers import TrainingArguments,Trainer

training_args = TrainingArguments(
    # The output directory where the model predictions and checkpoints will be written
    output_dir='./TTC4900Model',
    do_train=True,
    do_eval=True,
    #  The number of epochs, defaults to 3.0
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    # Number of steps used for a linear warmup
    warmup_steps=100,
    weight_decay=0.01,
    logging_strategy='steps',
   # TensorBoard log directory
    logging_dir='./multi-class-logs',
    logging_steps=50,
    evaluation_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    fp16=True,
    load_best_model_at_end=True
)

trainer = Trainer(
    # the pre-trained model that will be fine-tuned
    model=model,
     # training arguments that we defined above
    args=training_args,
    train_dataset=train_dataloader,
    eval_dataset=val_dataloader,
    compute_metrics= compute_metrics
)

trainer.train()

q=[trainer.evaluate(eval_dataset=df) for df in [train_dataloader, val_dataloader, test_dataset]]

pd.DataFrame(q, index=["train","val","test"]).iloc[:,:]

def predict(text):
    """
    Predicts the class label for a given input text

    Args:
        text (str): The input text for which the class label needs to be predicted.

    Returns:
        probs (torch.Tensor): Class probabilities for the input text.
        pred_label_idx (torch.Tensor): The index of the predicted class label.
        pred_label (str): The predicted class label.


    tokens = nltk.word_tokenize(text)

    tokens_pun_lower = [i.lower() for i in tokens if i.isalnum()]

    tokens_stop = [i for i in tokens_pun_lower if i not in stop_words]

    terms = [prt.stem(i) for i in tokens_stop]

    processed_text= " ".join(terms)"""

    # Tokenize the input text and move tensors to the GPU if available

    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors="pt").to("cuda")

    # Get model output (logits)
    outputs = model(**inputs)

    probs = outputs[0].softmax(1)
    """ Explanation outputs: The BERT model returns a tuple containing the output logits (and possibly other elements depending on the model configuration). In this case, the output logits are the first element in the tuple, which is why we access it using outputs[0].

    outputs[0]: This is a tensor containing the raw output logits for each class. The shape of the tensor is (batch_size, num_classes) where batch_size is the number of input samples (in this case, 1, as we are predicting for a single input text) and num_classes is the number of target classes.

    softmax(1): The softmax function is applied along dimension 1 (the class dimension) to convert the raw logits into class probabilities. Softmax normalizes the logits so that they sum to 1, making them interpretable as probabilities. """

    # Get the index of the class with the highest probability
    # argmax() finds the index of the maximum value in the tensor along a specified dimension.
    # By default, if no dimension is specified, it returns the index of the maximum value in the flattened tensor.
    pred_label_idx = probs.argmax()

    # Now map the predicted class index to the actual class label
    # Since pred_label_idx is a tensor containing a single value (the predicted class index),
    # the .item() method is used to extract the value as a scalar
    pred_label = model.config.id2label[pred_label_idx.item()]

    return probs, pred_label_idx, pred_label

model_path = "text-document-classification-model"
trainer.save_model(model_path)
tokenizer.save_pretrained(model_path)

text1=list(train_texts)[0]
text2=list(valid_texts)[0]
text3=list(test_texts)[0]


from transformers import pipeline
classifier = pipeline("text-classification", model="text-document-classification-model",tokenizer=model_path, max_length=512, truncation=True)
classifier(text1)

classifier(text2)

classifier(text3)

text4="It is normal for children to have trouble focusing and behaving at one time or another. However, children with ADHD do not just grow out of these behaviors. The symptoms continue, can be severe, and can cause difficulty at school, at home, or with friends."

classifier(text4)

text5="The Home Office reported that 90 per cent of arrests involved supporters of clubs in the top five English football divisions.'Prior to the 2019 to 2020 season, there was a downward trend in football-related arrests; a decrease of more than half (-55%) from 3,089 in the 2010 to 2011 season, to 1,381 in the 2018 to 2019 season (before COVID-19) the Home Office stated."

classifier(text5)

text6="The flavor was unexpectedly spicy, more akin to that of a dijon mustard than a classic yellow mustard, and the heat got more intense as I continued chewing. The tang hit the back of my throat and felt like I'd downed an oyster topped with too much horseradish. After swallowing, the artificial taste stuck around for longer than I'd hoped, along with the yellow dye on my tongue."
classifier(text6)

text7="The Fédération internationale de football association (French for 'International Association Football Federation';[3] abbreviated as FIFA and pronounced in English as /ˈfiːfə/) is the international governing body of association football, beach soccer, and futsal. It was founded in 1904[4] to oversee international competition among the national associations of Belgium, Denmark, France, Germany, the Netherlands, Spain, Sweden and Switzerland. Headquartered in Zürich, Switzerland, its membership now comprises 211 national associations. "

classifier(text7)

text8="Based on the rules of football, each team will have 11 players on the field.This means there will be a total of 22 players on the field for every play.This is the rule whether one team is on offense and the other defense, or whether it's a special teams play such as a punt, kickoff, or field goal attempt.Let's talk a little more about the three main 'units' on a football team, which are the offense, the defense, and the special teams."

classifier(text8)

text10="As the only peer-recognized music award, the GRAMMY is the music industry’s highest honor. We’re counting on you in the music community to continue that legacy by participating in the process."

classifier(text10)

